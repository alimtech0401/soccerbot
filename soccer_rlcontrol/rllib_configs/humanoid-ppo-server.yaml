humanoid-ppo:
    env: gym_soccerbot:walk-forward-v1
    run: PPO
    checkpoint_freq: 20
    max_failures: 10
    checkpoint_at_end: true
    local_dir: ./results/
    stop:
        timesteps_total: 1000000000
    config:
        # Works for both torch and tf.
        framework: torch
        num_workers: 32
        num_cpus_per_worker: 0
        num_envs_per_worker: 4
        horizon: 4096
        num_gpus: 1

        # Should use a critic as a baseline (otherwise don't use value baseline;
        # required for using GAE).
        use_critic: True
        # If true use the Generalized Advantage Estimator (GAE)
        # with a value function see https://arxiv.org/pdf/1506.02438.pdf.
        use_gae: True
        # The GAE (lambda) parameter.
        lambda: 1.0
        # Initial coefficient for KL divergence.
        kl_coeff: 0.2
        # Size of batches collected from each worker.
        rollout_fragment_length: 4096
        # Number of timesteps collected for each SGD round. This defines the size
        # of each SGD epoch.
        train_batch_size: 524288 #4096*32*4 # 4000
        # Total SGD batch size across all devices for SGD. This defines the
        # minibatch size within each epoch.
        sgd_minibatch_size: 524288 #4096*32*4 # 128
        # Whether to shuffle sequences in the batch when training (recommended).
        shuffle_sequences: True
        # Number of SGD iterations in each outer loop (i.e. number of epochs to
        # execute per train batch).
        num_sgd_iter: 30
        # Stepsize of SGD.
        lr: !!float 5e-5
        # Learning rate schedule.
        lr_schedule: null
        # Coefficient of the value function loss. IMPORTANT: you must tune this if
        # you set vf_share_layers=True inside your model's config.
        vf_loss_coeff: 1.0
        model:
            fcnet_hiddens: [256, 256]
            fcnet_activation: relu
            free_log_std: true
            # Share layers for value function. If you set this to True it's
            # important to tune vf_loss_coeff.
            vf_share_layers: False
        # Coefficient of the entropy regularizer.
        entropy_coeff: 0.0
        # Decay schedule for the entropy regularizer.
        entropy_coeff_schedule: null
        # PPO clip parameter.
        clip_param: 0.3
        # Clip param for the value function. Note that this is sensitive to the
        # scale of the rewards. If your expected V is large increase this.
        vf_clip_param: 10.0
        # If specified clip the global norm of gradients by this amount.
        grad_clip: null
        # Target value for KL divergence.
        kl_target: 0.01
        # Whether to rollout complete_episodes or truncate_episodes.
        batch_mode: truncate_episodes
        # Which observation filter to apply to the observation.
        observation_filter: NoFilter
